# LangSmith Screenshot Capture Guide

**Purpose:** Capture evaluation traces showing Base vs Hybrid+Rerank retrievers for documentation.

---

## ğŸ¯ What to Capture

You need **2 screenshots** showing:
1. Base retriever evaluation trace
2. Hybrid+Rerank retriever evaluation trace

Each screenshot should show:
- âœ… Query input
- âœ… Retrieved document chunks (contexts)
- âœ… Generated answer
- âœ… Semantic scoring metrics (if logged)
- âœ… Execution time
- âœ… Token usage

---

## ğŸ“‹ Step-by-Step Instructions

### Step 1: Access LangSmith Dashboard

1. Go to: https://smith.langchain.com/
2. Log in with your account
3. Select your project: **"MoneyMentor"**

### Step 2: Find Base Retriever Runs

**Option A: From Recent Evaluation**
1. Click on "Traces" in the left sidebar
2. Filter by tags:
   - Tag: `moneymentor`
   - Tag: `semantic_evaluation_tfidf`
   - Tag: `retriever=base`
3. Look for runs named: `"Semantic_Eval_base_simple"` or `"Semantic_Eval_base_reasoning"`

**Option B: Search by Date**
1. Filter runs from: **October 19, 2025 (today)**
2. Look for runs around: **22:30-23:00**
3. Find runs with "base" in the name

### Step 3: Select a Good Example Run

**Choose a run that shows:**
- âœ… A complex query (preferably from reasoning dataset)
- âœ… Clear retrieved contexts (5 chunks visible)
- âœ… Complete generated answer
- âœ… All metadata visible

**Good example queries:**
- "Compare traditional IRA vs Roth IRA..."
- "How do compound interest and inflation interact..."
- "Explain how diversification reduces portfolio risk..."

### Step 4: Capture Base Retriever Screenshot

1. Click on the selected run to open details
2. Expand all relevant sections:
   - **Inputs:** Should show the query
   - **Outputs:** Should show the answer and metrics
   - **Metadata:** Should show retrieval mode, contexts
3. Scroll to fit as much information as possible
4. Take screenshot (Cmd+Shift+4 on Mac, Win+Shift+S on Windows)
5. Save as: `docs/images/langsmith_trace_base.png`

**What should be visible:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run: Semantic_Eval_base_reasoning       â”‚
â”‚ Status: âœ“ Success                       â”‚
â”‚ Duration: ~1.2s                         â”‚
â”‚                                         â”‚
â”‚ ğŸ“¥ INPUTS                               â”‚
â”‚ query: "Compare traditional IRA vs..."  â”‚
â”‚ mode: "base"                            â”‚
â”‚ dataset: "reasoning"                    â”‚
â”‚                                         â”‚
â”‚ ğŸ“¤ OUTPUTS                              â”‚
â”‚ generated_answer: "Traditional IRAs..." â”‚
â”‚ num_contexts: 5                         â”‚
â”‚ faithfulness: 0.156                     â”‚
â”‚ relevancy: 0.241                        â”‚
â”‚ precision: 0.023                        â”‚
â”‚ recall: 0.048                           â”‚
â”‚                                         â”‚
â”‚ ğŸ” RETRIEVED CONTEXTS (sample)          â”‚
â”‚ 1. "A traditional IRA allows..."        â”‚
â”‚ 2. "Roth IRA contributions are..."      â”‚
â”‚ ...                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 5: Find Hybrid+Rerank Retriever Runs

1. Go back to "Traces"
2. Filter by tags:
   - Tag: `moneymentor`
   - Tag: `semantic_evaluation_tfidf`
   - Tag: `retriever=hybrid_rerank`
3. Look for runs named: `"Semantic_Eval_advanced_reasoning"`

**Tip:** Try to find a run with the **same query** as your Base screenshot for easy comparison!

### Step 6: Capture Hybrid+Rerank Screenshot

1. Click on the selected run
2. Expand all sections (Inputs, Outputs, Metadata)
3. Take screenshot
4. Save as: `docs/images/langsmith_trace_hybrid.png`

**What should be visible:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run: Semantic_Eval_advanced_reasoning   â”‚
â”‚ Status: âœ“ Success                       â”‚
â”‚ Duration: ~1.8s                         â”‚
â”‚                                         â”‚
â”‚ ğŸ“¥ INPUTS                               â”‚
â”‚ query: "Compare traditional IRA vs..."  â”‚
â”‚ mode: "advanced"                        â”‚
â”‚ dataset: "reasoning"                    â”‚
â”‚                                         â”‚
â”‚ ğŸ“¤ OUTPUTS                              â”‚
â”‚ generated_answer: "Traditional IRAs..." â”‚
â”‚ num_contexts: 5                         â”‚
â”‚ faithfulness: 0.162 (â†‘ vs base)         â”‚
â”‚ relevancy: 0.248 (â†‘ vs base)            â”‚
â”‚ precision: 0.023                        â”‚
â”‚ recall: 0.048                           â”‚
â”‚                                         â”‚
â”‚ ğŸ” RETRIEVED CONTEXTS (sample)          â”‚
â”‚ 1. "Traditional IRA tax treatment..."   â”‚
â”‚ 2. "Roth IRA advantages include..."     â”‚
â”‚ ...                                     â”‚
â”‚                                         â”‚
â”‚ ğŸš€ HYBRID RETRIEVAL DETAILS             â”‚
â”‚ BM25 matches: 20                        â”‚
â”‚ Vector matches: 20                      â”‚
â”‚ Ensemble results: 24                    â”‚
â”‚ Reranked to: 5                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ Screenshot Best Practices

### Do's âœ…
- âœ… Use high resolution (at least 1920x1080)
- âœ… Expand all relevant sections
- âœ… Show complete query and answer
- âœ… Include all metrics
- âœ… Capture in light mode (better readability)
- âœ… Crop to remove unnecessary UI elements

### Don'ts âŒ
- âŒ Don't capture partial information
- âŒ Don't use low resolution
- âŒ Don't include sensitive API keys
- âŒ Don't show unrelated runs or projects

---

## ğŸ” Alternative: What If Runs Don't Show Metrics?

**Problem:** The semantic evaluation script had LangSmith logging issues (run_type validation error).

**Solution 1: Use evaluation JSON files**
```bash
# View detailed results
cat reports/semantic_evaluation_reasoning_*.json | jq '.base_retriever[] | select(.query | contains("IRA"))'
```

**Solution 2: Create comparison table manually**
Use the data from `reports/semantic_evaluation_*.json` to create a comparison table in markdown.

**Solution 3: Run evaluation again with fixed LangSmith logging**
1. Fix the `run_type` in `scripts/evaluate_semantic_simple.py`
2. Change `run_type="evaluation"` to `run_type="chain"`
3. Re-run evaluation: `python scripts/evaluate_semantic_simple.py --dataset reasoning`

---

## ğŸ¨ Alternative: Create Comparison Diagram

If screenshots are difficult, create a side-by-side comparison diagram showing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BASE          â”‚  HYBRID+RERANK  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mode: base      â”‚ Mode: advanced  â”‚
â”‚ Latency: 0.8s   â”‚ Latency: 1.8s   â”‚
â”‚ Cost: $0.00002  â”‚ Cost: $0.00025  â”‚
â”‚                 â”‚                 â”‚
â”‚ Faithfulness:   â”‚ Faithfulness:   â”‚
â”‚   0.145         â”‚   0.156 (+1.1%) â”‚
â”‚                 â”‚                 â”‚
â”‚ Relevancy:      â”‚ Relevancy:      â”‚
â”‚   0.230         â”‚   0.241 (+1.1%) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Checklist

Before moving forward, ensure you have:

- [ ] Accessed LangSmith dashboard
- [ ] Found Base retriever runs
- [ ] Found Hybrid+Rerank retriever runs
- [ ] Captured Base screenshot (langsmith_trace_base.png)
- [ ] Captured Hybrid+Rerank screenshot (langsmith_trace_hybrid.png)
- [ ] Both screenshots show the same query (for comparison)
- [ ] Screenshots are high quality and readable
- [ ] Screenshots saved in `docs/images/`

---

## ğŸš€ Next Steps After Capturing

Once you have the screenshots:

1. Verify they're saved in the correct location:
   ```bash
   ls -lh docs/images/langsmith_trace_*.png
   ```

2. Update documentation to embed them:
   ```markdown
   **Base Retriever Trace:**
   ![Base Retriever LangSmith Trace](images/langsmith_trace_base.png)
   
   **Hybrid+Rerank Retriever Trace:**
   ![Hybrid+Rerank LangSmith Trace](images/langsmith_trace_hybrid.png)
   ```

3. Commit the images:
   ```bash
   git add docs/images/langsmith_trace_*.png
   git commit -m "docs: Add LangSmith evaluation trace screenshots"
   ```

---

## ğŸ’¡ Tips

- **Best query to use:** "Compare traditional IRA vs Roth IRA..." (shows multi-source synthesis)
- **Best time to capture:** Right after running evaluation (traces are fresh)
- **Best view:** Expand all sections but keep it concise (single scrollable view)
- **Format:** PNG for best compatibility with markdown

---

## â“ Troubleshooting

**Issue:** Can't find runs in LangSmith
- **Solution:** Check if `LANGCHAIN_TRACING_V2=true` in your `.env`
- **Solution:** Verify `LANGCHAIN_API_KEY` is set correctly
- **Solution:** Re-run evaluation to generate fresh traces

**Issue:** Runs show error status
- **Solution:** Check the error message in LangSmith
- **Solution:** The validation error is known (run_type issue)
- **Solution:** Use the JSON reports as alternative

**Issue:** Screenshots too large
- **Solution:** Crop unnecessary parts
- **Solution:** Compress using: `convert input.png -quality 85 output.png`

---

**Document Version:** 1.0  
**Last Updated:** October 19, 2025  
**Status:** Ready for screenshot capture

